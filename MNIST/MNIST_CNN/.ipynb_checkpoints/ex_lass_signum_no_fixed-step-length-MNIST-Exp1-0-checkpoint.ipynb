{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading mnist\n",
      "\n",
      "Shuffling training data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = '/home/yash/Desktop/NUS_intern/tensorflow-adversarial/tf_example'\n",
    "os.chdir(path)\n",
    "# supress tensorflow logging other than errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import ModeKeys, Estimator\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from fgsm4 import fgsm\n",
    "import mnist\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "img_chas = 1\n",
    "input_shape = (img_rows, img_cols, img_chas)\n",
    "n_classes = 10\n",
    "\n",
    "print('\\nLoading mnist')\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train.reshape(-1, img_rows, img_cols, img_chas)\n",
    "X_test = X_test.reshape(-1, img_rows, img_cols, img_chas)\n",
    "\n",
    "# one hot encoding, basically creates hte si\n",
    "def _to_categorical(x, n_classes):\n",
    "    x = np.array(x, dtype=int).ravel()\n",
    "    n = x.shape[0]\n",
    "    ret = np.zeros((n, n_classes))\n",
    "    ret[np.arange(n), x] = 1\n",
    "    return ret\n",
    "\n",
    "def find_l2(X_test, X_adv):\n",
    "    a=X_test.reshape(-1,28*28)\n",
    "    b=X_adv.reshape(-1,28*28)\n",
    "    l2_unsquared = np.sum(np.square(a-b),axis=1)\n",
    "    return l2_unsquared\n",
    "\n",
    "y_train = _to_categorical(y_train, n_classes)\n",
    "y_test = _to_categorical(y_test, n_classes)\n",
    "print('\\nShuffling training data')\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "# X_train = X_train[:1000]\n",
    "# y_train = y_train[:1000]\n",
    "\n",
    "\n",
    "# split training/validation dataset\n",
    "validation_split = 0.1\n",
    "n_train = int(X_train.shape[0]*(1-validation_split))\n",
    "X_valid = X_train[n_train:]\n",
    "X_train = X_train[:n_train]\n",
    "y_valid = y_train[n_train:]\n",
    "y_train = y_train[:n_train]\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "env = Dummy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(x, logits=False, training=False):\n",
    "    conv0 = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(pool0, filters=64,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "  \n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    \n",
    "    flat = tf.reshape(pool1, [-1, 7*7*64], name='flatten')\n",
    "    dense1 = tf.layers.dense(flat, units=1024, activation=tf.nn.relu,\n",
    "                            name='dense1')\n",
    "    dense2 = tf.layers.dense(dense1, units=128, activation=tf.nn.relu,\n",
    "                            name='dense2')\n",
    "    logits_ = tf.layers.dense(dense2, units=10, name='logits') #removed dropout\n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model'):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,\n",
    "                                        img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True,\n",
    "                             training=env.training)\n",
    "\n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    env.count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(env.count, name='acc')\n",
    "\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
    "                                                   logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    env.optim = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08,).minimize(env.loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.x_adv, env.all_flipped = fgsm(model, env.x, step_size=.05, bbox_semi_side=10) #epochs is redundant now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,  './models/mnist/' + label)\n",
    "    \n",
    "def restore_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/mnist/' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_count, batch_acc = sess.run(\n",
    "            [env.loss, env.count, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        print('batch count: {0}'.format(np.sum(batch_count)))\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= ns\n",
    "    acc /= ns\n",
    "#     print (ns)\n",
    "#     print (n_sample)\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    return yval\n",
    "\n",
    "def train(label):\n",
    "    print('\\nTraining')\n",
    "    n_sample = X_train.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 50\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_sample, start+batch_size)\n",
    "            sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                           env.y: y_train[start:end],\n",
    "                                           env.training: True})\n",
    "        if(epoch%5 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            print(\"saving model \" + model_label)\n",
    "            save_model(model_label)\n",
    "            \n",
    "    save_model(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_adv(X, Y, label):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    X_adv = np.empty_like(X)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp, all_flipped = sess.run([env.x_adv, env.all_flipped], feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "#         _evaluate(tmp, Y[start:end],env)\n",
    "        X_adv[start:end] = tmp\n",
    "#         print(all_flipped)\n",
    "    print('\\nSaving adversarial')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    np.save('data/mnist/' + label + '.npy', X_adv)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mnist/mnist_with_cnn0\n",
      "\n",
      "Evaluating\n",
      "batch count: 123.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 122.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 122.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 122.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 121.0\n",
      "batch count: 123.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 122.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 122.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 124.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 123.0\n",
      "batch count: 123.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 123.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 123.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 128.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 128.0\n",
      "batch count: 128.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 128.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 126.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 124.0\n",
      "batch count: 123.0\n",
      "batch count: 124.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 125.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 125.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 126.0\n",
      "batch count: 125.0\n",
      "batch count: 122.0\n",
      "batch count: 126.0\n",
      "batch count: 126.0\n",
      "batch count: 123.0\n",
      "batch count: 127.0\n",
      "batch count: 127.0\n",
      "batch count: 108.0\n",
      " loss: 0.0555 acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.055477409997637166, 0.98205029201733562)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"mnist_with_cnn\"\n",
    "# train(label) # else \n",
    "#Assuming that you've started a session already else do that first!\n",
    "restore_model(label + '0')\n",
    "# restore_model(label + '10')\n",
    "# restore_model(label + '50')\n",
    "# restore_model(label + '100')\n",
    "_evaluate(X_train, y_train, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def random_normal_func(X):\n",
    "    X=X.reshape(-1,28*28)\n",
    "    print(X.shape)\n",
    "    mean, std = np.mean(X, axis=0), np.std(X,axis=0)\n",
    "    randomX = np.zeros([10000,X[0].size])\n",
    "    print(randomX.shape)\n",
    "    for i in range(X[0].size):\n",
    "        randomX[:,i] = np.random.normal(mean[i],std[i],10000)\n",
    "    randomX = randomX.reshape(-1,28,28,1)\n",
    "    ans = sess.run(env.ybar, feed_dict={env.x: randomX,env.training: False})\n",
    "    labels = _to_categorical(np.argmax(ans,axis=1), n_classes)\n",
    "    return randomX,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784)\n",
      "(10000, 784)\n",
      "\n",
      "Crafting adversarial\n",
      " batch 262/10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b206142de43f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mX_random_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_random_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_normal_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX_adv_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mX_adv_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mX_adv_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_random\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a854f5faf70f>\u001b[0m in \u001b[0;36mcreate_adv\u001b[0;34m(X, Y, label)\u001b[0m\n\u001b[1;32m     12\u001b[0m         tmp, all_flipped = sess.run([env.x_adv, env.all_flipped], feed_dict={env.x: X[start:end],\n\u001b[1;32m     13\u001b[0m                                              \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                              env.training: False})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         _evaluate(tmp, Y[start:end],env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mX_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = \"test_fs\"\n",
    "train = \"train_fs\"\n",
    "random = \"random_fs\"\n",
    "random_normal= \"random_normal_fs\"\n",
    "\n",
    "\n",
    "X_train_sub = X_train[:10000]\n",
    "y_train_sub = sess.run(env.ybar, feed_dict={env.x: X_train_sub,env.training: False})\n",
    "y_train_sub = _to_categorical(np.argmax(y_train_sub, axis=1), n_classes)\n",
    "\n",
    "y_test_sub = sess.run(env.ybar, feed_dict={env.x: X_test,env.training: False})\n",
    "y_test_sub = _to_categorical(np.argmax(y_test_sub, axis=1), n_classes)\n",
    "\n",
    "X_random = np.random.rand(10000,28,28,1)\n",
    "X_random = X_random[:10000]\n",
    "y_random = sess.run(env.ybar, feed_dict={env.x: X_random,env.training: False})\n",
    "y_random = _to_categorical(np.argmax(y_random, axis=1), n_classes)\n",
    "X_random_normal, y_random_normal = random_normal_func(X_train)\n",
    "\n",
    "X_adv_test = create_adv(X_test, y_test_sub, test)\n",
    "X_adv_train = create_adv(X_train_sub, y_train_sub, train)\n",
    "X_adv_random = create_adv(X_random,y_random, random)\n",
    "X_adv_random_normal = create_adv(X_random_normal, y_random_normal, random_normal)\n",
    "\n",
    "# X_adv_test = np.load('data/mnist/' + test + '.npy')\n",
    "# X_adv_train = np.load('data/mnist/' + train + '.npy')\n",
    "# X_adv_random = np.load('data/mnist/' + random + '.npy')\n",
    "# X_adv_random_normal = np.load('data/mnist/' + random_normal + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l2_test = find_l2(X_adv_test,X_test)\n",
    "l2_train = find_l2(X_adv_train, X_train_sub)\n",
    "l2_random = find_l2(X_adv_random,X_random)\n",
    "l2_random_normal = find_l2(X_adv_random_normal,X_random_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(l2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(X_adv_random_normal[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(1,10001, 1)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, l2_test, 'r--', t, l2_train, 'b--', t, l2_random, 'y--', l2_random_normal, 'g--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "%matplotlib inline\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(1,101, 1)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, l2_test[:100], 'r--', t, l2_train[:100], 'b--',t, l2_random[:100], 'y--',l2_random_normal[:100], 'g--')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Train Data')\n",
    "plt.legend(handles=[blue_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(l2_test,100)\n",
    "plt.title(\"L2 distance of test data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(l2_train,100)\n",
    "plt.title(\"L2 distance of train data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(l2_random,100)\n",
    "plt.title(\"L2 distance of random data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(l2_random_normal,100)\n",
    "plt.title(\"L2 distance of random normal data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16.0,
     27.0,
     38.0,
     58.0,
     64.0,
     67.0,
     73.0,
     88.0,
     93.0,
     98.0,
     102.0,
     107.0,
     110.0,
     117.0,
     119.0,
     128.0,
     132.0,
     137.0,
     142.0,
     146.0,
     161.0,
     170.0,
     176.0,
     183.0,
     202.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/yash/Desktop/Decision-Flip-Experiments')\n",
    "from scipy.misc import imread\n",
    "import matplotlib.patches as mpatches\n",
    "from models import *\n",
    "from plotter import *\n",
    "from saveloader import *\n",
    "from fgsm_cifar import fgsm\n",
    "from fgsm_cifar_wrt_class import fgsm_wrt_class\n",
    "from helper import *\n",
    "from helper import _to_categorical\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "sd = 'shape_dict'\n",
    "\n",
    "\n",
    "def plot_all_data_graph(method,epochs,n,lyr):\n",
    "    for from_cls in range(2):\n",
    "        for to_cls in range(n_classes):\n",
    "            if(from_cls != to_cls):\n",
    "                l2_test, l2_train = restore_flip(method,epochs,n,from_cls,to_cls,lyr)\n",
    "                #There might be a prob. here! since lens are diff, solved it inside the func\n",
    "                plot_data_graph_without_random(l2_test, l2_train, n,from_cls,to_cls,lyr)\n",
    "                plot_hists_without_random(l2_test, l2_train, n,from_cls,to_cls,lyr)\n",
    "                \n",
    "\n",
    "\n",
    "def make_data(n):\n",
    "    X_test_sub = X_test[:n]\n",
    "    X_train_sub = X_train[:n]\n",
    "    y_train_sub = sess.run(env.ybar, feed_dict={env.x: X_train_sub,env.training: False})\n",
    "    y_train_sub = _to_categorical(np.argmax(y_train_sub, axis=1), n_classes)\n",
    "    y_test_sub = sess.run(env.ybar, feed_dict={env.x:X_test_sub ,env.training: False})\n",
    "    y_test_sub = _to_categorical(np.argmax(y_test_sub, axis=1), n_classes)\n",
    "    \n",
    "    return X_test_sub, y_test_sub, X_train_sub, y_train_sub\n",
    "\n",
    "\n",
    "def random_normal_func(X, n, save, lr, lrn):\n",
    "    X=X.reshape(-1,img_rows*img_cols*img_chas)\n",
    "    mean, std = np.mean(X, axis=0), np.std(X,axis=0)\n",
    "    randomX = np.zeros([n,X[0].size])\n",
    "    for i in range(X[0].size):\n",
    "        randomX[:,i] = np.random.normal(mean[i],std[i],n)\n",
    "    randomX = randomX.reshape(-1,img_rows,img_cols,img_chas)\n",
    "    X_random_normal = randomX\n",
    "    ans = sess.run(env.ybar, feed_dict={env.x: randomX,env.training: False})\n",
    "    y_random_normal = _to_categorical(np.argmax(ans,axis=1), n_classes)\n",
    "    X_random = np.random.rand(n,img_rows,img_cols,img_chas)\n",
    "    y_random = sess.run(env.ybar, feed_dict={env.x: X_random,env.training: False})\n",
    "    y_random = _to_categorical(np.argmax(y_random, axis=1), n_classes)\n",
    "    \n",
    "    if(save):\n",
    "        save_as_txt(get_flip_path(lr),X_random)\n",
    "        save_as_txt(get_flip_path(lrn),X_random_normal)\n",
    "    \n",
    "    return X_random, y_random, X_random_normal, y_random_normal  \n",
    "\n",
    "def run_flip(method, epochs, n,from_cls=-1, to_cls = -1, layer= -1):\n",
    "    save_obj({},sd)\n",
    "    test_label = make_label(\"test\", method, epochs,n, False)\n",
    "    train_label = make_label(\"train\", method, epochs,n, False)\n",
    "    random_label = make_label(\"random\", method, epochs,n, False)\n",
    "    random_normal_label = make_label(\"random_normal\", method, epochs,n, False)\n",
    "#     data_label_random, data_label_random_normal = make_label(\"_\", method, epochs,n, True)\n",
    "    \n",
    "    X_test_sub, y_test_sub, X_train_sub, y_train_sub = make_data(n)\n",
    "#     X_random, y_random, X_random_normal, y_random_normal = restore_random_data(data_label_random, data_label_random_normal)\n",
    "    \n",
    "    if(method==2):\n",
    "        X_flip_per_class_test = create_adv_wrt_class(X_test_sub, y_test_sub, test_label)\n",
    "        X_flip_per_class_train = create_adv_wrt_class(X_train_sub, y_train_sub, train_label)\n",
    "#         X_flip_per_class_random = create_adv_wrt_class(X_random, y_random, random_label)\n",
    "#         X_flip_per_class_random_normal = create_adv_wrt_class(X_random_normal, y_random_normal, random_normal_label)\n",
    "        \n",
    "    if(from_cls != -1):\n",
    "            print('From Class ' + str(from_cls) + '\\n')\n",
    "            X_test_sub, y_test_sub, X_flip_per_class_test = get_class(X_test_sub, y_test_sub,\n",
    "                                                                     X_flip_per_class_test, from_cls)\n",
    "            X_train_sub, y_train_sub, X_flip_per_class_train= get_class(X_train_sub, y_train_sub,\n",
    "                                                                       X_flip_per_class_train, from_cls)\n",
    "#             X_random, y_random, X_flip_per_class_random = get_class(X_random, y_random, \n",
    "#                                                                     X_flip_per_class_random, from_cls)\n",
    "#             X_random_normal, y_random_normal, X_flip_per_class_random_normal = get_class(\n",
    "#                 X_random_normal, y_random_normal, X_flip_per_class_random_normal, from_cls)\n",
    "            print('Test Data:' + str(y_test_sub.shape[0]))\n",
    "            print('Train Data: ' + str(y_train_sub.shape[0]))\n",
    "#             print('Random Data: ' + str(y_random.shape[0]))\n",
    "#             print('Random Normal Data: ' + str(y_random_normal.shape[0]))\n",
    "        \n",
    "    _, X_flip_test = give_m2_ans(X_test_sub, X_flip_per_class_test, to_cls)\n",
    "    _, X_flip_train = give_m2_ans(X_train_sub, X_flip_per_class_train, to_cls)\n",
    "#     _, X_flip_random = give_m2_ans(X_random,X_flip_per_class_random, to_cls)\n",
    "#     _, X_flip_random_normal = give_m2_ans(X_random_normal, X_flip_per_class_random_normal,to_cls)\n",
    "\n",
    "    l2_test = get_l2_at_layer(X_flip_test,X_test_sub, layer)\n",
    "    l2_train = get_l2_at_layer(X_flip_train, X_train_sub, layer)\n",
    "#     l2_random = get_l2_at_layer(X_flip_random,X_random, layer)\n",
    "#     l2_random_normal = get_l2_at_layer(X_flip_random_normal,X_random_normal, layer)\n",
    "\n",
    "    return l2_test, l2_train\n",
    "\n",
    "def restore_flip(method, epochs, n,from_cls=-1, to_cls = -1, layer= -1):\n",
    "    test_label = make_label(\"test\", method, epochs,n, False)\n",
    "    train_label = make_label(\"train\", method, epochs,n, False)\n",
    "    random_label = make_label(\"random\", method, epochs,n, False)\n",
    "    random_normal_label = make_label(\"random_normal\", method, epochs,n, False)\n",
    "#     data_label_random, data_label_random_normal = make_label(\"_\", method, epochs,n, True)\n",
    "    \n",
    "    X_test_sub, y_test_sub, X_train_sub, y_train_sub = make_data(n)\n",
    "#     X_random, y_random, X_random_normal, y_random_normal = restore_random_data(data_label_random, data_label_random_normal)\n",
    "    \n",
    "    \n",
    "    if(method==2):\n",
    "        X_flip_per_class_test = load_from_txt(get_flip_path(test_label))\n",
    "        X_flip_per_class_train = load_from_txt(get_flip_path(train_label))\n",
    "#         X_flip_per_class_random = load_from_txt(get_flip_path(random_label))\n",
    "#         X_flip_per_class_random_normal = load_from_txt(get_flip_path(random_normal_label))\n",
    "            \n",
    "        if(from_cls != -1):\n",
    "            print('From Class ' + str(from_cls) + '\\n')\n",
    "            X_test_sub, y_test_sub, X_flip_per_class_test = get_class(X_test_sub, y_test_sub,\n",
    "                                                                     X_flip_per_class_test, from_cls)\n",
    "            X_train_sub, y_train_sub, X_flip_per_class_train= get_class(X_train_sub, y_train_sub,\n",
    "                                                                       X_flip_per_class_train, from_cls)\n",
    "#             X_random, y_random, X_flip_per_class_random = get_class(X_random, y_random, \n",
    "#                                                                     X_flip_per_class_random, from_cls)\n",
    "#             X_random_normal, y_random_normal, X_flip_per_class_random_normal = get_class(\n",
    "#                 X_random_normal, y_random_normal, X_flip_per_class_random_normal, from_cls)\n",
    "            print('Test Data:' + str(y_test_sub.shape[0]))\n",
    "            print('Train Data: ' + str(y_train_sub.shape[0]))\n",
    "#             print('Random Data: ' + str(y_random.shape[0]))\n",
    "#             print('Random Normal Data: ' + str(y_random_normal.shape[0]))\n",
    "            \n",
    "    _, X_flip_test = give_m2_ans(X_test_sub, X_flip_per_class_test, to_cls)\n",
    "    _, X_flip_train = give_m2_ans(X_train_sub, X_flip_per_class_train, to_cls)\n",
    "#     _, X_flip_random = give_m2_ans(X_random,X_flip_per_class_random, to_cls)\n",
    "#     _, X_flip_random_normal = give_m2_ans(X_random_normal, X_flip_per_class_random_normal,to_cls)\n",
    "\n",
    "    l2_test = get_l2_at_layer(X_flip_test,X_test_sub, layer)\n",
    "    l2_train = get_l2_at_layer(X_flip_train, X_train_sub, layer)\n",
    "#     l2_random = get_l2_at_layer(X_flip_random,X_random, layer)\n",
    "#     l2_random_normal = get_l2_at_layer(X_flip_random_normal,X_random_normal, layer)\n",
    "\n",
    "    return l2_test, l2_train\n",
    "\n",
    "def plot_data_graph(l2_test, l2_train, l2_random, l2_random_normal, n, from_cls, to_cls):\n",
    "    %matplotlib inline\n",
    "    t = np.arange(1,n+1, 1)\n",
    "    plt.plot(t, l2_test[:n], 'r--', t, l2_train[:n],'b--', t, l2_random[:n], 'y--', l2_random_normal[:n], 'k--')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Train Data')\n",
    "    red_patch = mpatches.Patch(color='red', label='Test Data')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='Random Data')\n",
    "    black_patch = mpatches.Patch(color='black', label='Random Normal Data')\n",
    "    plt.legend(handles=[blue_patch, red_patch, yellow_patch, black_patch])\n",
    "    plt.title(\"From \" + str(from_cls) + \" to \" + str(to_cls))\n",
    "    plt.xlabel(\"Examples\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "def plot_data_hist(l2,n,title):\n",
    "    %matplotlib inline\n",
    "    plt.hist(l2,n)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_hists_without_random(l2_test, l2_train, n,from_cls,to_cls,lyr):\n",
    "    plot_data_hist(l2_test, l2_test.shape[0], \"Train Data\\n From \" + str(from_cls) + \" to \" + str(to_cls) + \" layer \" + str(lyr))\n",
    "    plot_data_hist(l2_train, l2_train.shape[0], \"Test Data\\n From \" + str(from_cls) + \" to \" + str(to_cls)+ \" layer \" + str(lyr))\n",
    "    \n",
    "    \n",
    "# one hot encoding, basically creates hte si\n",
    "def _to_categorical(x, n_classes):\n",
    "    x = np.array(x, dtype=int).ravel()\n",
    "    n = x.shape[0]\n",
    "    ret = np.zeros((n, n_classes))\n",
    "    ret[np.arange(n), x] = 1\n",
    "    return ret\n",
    "\n",
    "def plot_data_graph_without_random(l2_test, l2_train, n, from_cls, to_cls,lyr):\n",
    "    %matplotlib inline\n",
    "    n = min(l2_test.shape[0], l2_train.shape[0])\n",
    "    t = np.arange(1,n+1, 1)\n",
    "    plt.plot(t, l2_test[:n], 'r--', t, l2_train[:n],'b--')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Train Data')\n",
    "    red_patch = mpatches.Patch(color='red', label='Test Data')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='Random Data')\n",
    "    black_patch = mpatches.Patch(color='black', label='Random Normal Data')\n",
    "    plt.legend(handles=[blue_patch, red_patch, yellow_patch, black_patch])\n",
    "    plt.title(\"From \" + str(from_cls) + \" to \" + str(to_cls)+ \" layer \" + str(lyr))\n",
    "    plt.xlabel(\"Examples\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    \n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def restore_random_data(lr, lrn):\n",
    "    Xr = load_from_txt(get_flip_path(lr))\n",
    "    Xrn = load_from_txt(get_flip_path(lrn))\n",
    "    y_random_normal = sess.run(env.ybar, feed_dict={env.x: Xrn,env.training: False})\n",
    "    y_random_normal = _to_categorical(np.argmax(y_random_normal,axis=1), n_classes)\n",
    "    y_random = sess.run(env.ybar, feed_dict={env.x: Xr,env.training: False})\n",
    "    y_random = _to_categorical(np.argmax(y_random, axis=1), n_classes)\n",
    "    \n",
    "    return Xr, y_random, Xrn, y_random_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('\\nLoading CIFAR10')\n",
    "ab=sys.getdefaultencoding()\n",
    "print(ab)\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train.reshape(-1, img_rows, img_cols, img_chas)\n",
    "X_test = X_test.reshape(-1, img_rows, img_cols, img_chas)\n",
    "\n",
    "y_train = _to_categorical(y_train, n_classes)\n",
    "y_test = _to_categorical(y_test, n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12.0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('\\nLoading pre-Shuffled training data')\n",
    "ind = np.loadtxt(get_misc_path('ind'), dtype = int)\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "# split training/validation dataset\n",
    "validation_split = 0.1\n",
    "n_train = int(X_train.shape[0]*(1-validation_split))\n",
    "X_valid = X_train[n_train:]\n",
    "X_train = X_train[:n_train]\n",
    "y_valid = y_train[n_train:]\n",
    "y_train = y_train[:n_train]\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "env = Dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     1.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(x, logits=False, training=False, layer = -1):\n",
    "    conv0 = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(pool0, filters=64,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "   \n",
    " \n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(pool1, filters=128,\n",
    "                             kernel_size=[1,1], padding='same',\n",
    "                             name='conv2', activation=tf.nn.relu)\n",
    "    \n",
    "\n",
    "    \n",
    "    flat = tf.reshape(conv2, [-1, 8*8*128], name='flatten')\n",
    "    \n",
    "    dense1 = tf.layers.dense(flat, units= 1024, activation=tf.nn.relu,\n",
    "                            name='dense1')\n",
    "    \n",
    "    dense2 = tf.layers.dense(dense1, units=128, activation=tf.nn.relu,\n",
    "                            name='dense2')\n",
    "    logits_ = tf.layers.dense(dense2, units=10, name='logits') #removed dropout\n",
    "    \n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "    \n",
    "    \n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    \n",
    "    y = tf.cond(tf.equal(layer, tf.constant(-1)), lambda: y, lambda: y)\n",
    "    y = tf.cond(tf.equal(layer, tf.constant(0)), lambda: x, lambda: y)\n",
    "    y = tf.cond(tf.equal(layer, tf.constant(1)), lambda: pool0, lambda: y)\n",
    "    y = tf.cond(tf.equal(layer, tf.constant(2)), lambda: pool1, lambda: y)\n",
    "    y = tf.cond(tf.equal(layer, tf.constant(3)), lambda: dense1, lambda: y)\n",
    "    y = tf.cond(tf.equal(layer, tf.constant(4)), lambda: dense2, lambda: y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model'):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,\n",
    "                                        img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True,\n",
    "                             training=env.training)\n",
    "    \n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    env.count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(env.count, name='acc')\n",
    "\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
    "                                                   logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    env.optim = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08,).minimize(env.loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.lyr= tf.placeholder(tf.int32)\n",
    "    env.layer_out = model(env.x, layer=env.lyr, logits=False ,training = env.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     4.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,  './models/cifar/' + label)\n",
    "    \n",
    "def restore_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/cifar/' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     23.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_count, batch_acc = sess.run(\n",
    "            [env.loss, env.count, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        print('batch count: {0}'.format(np.sum(batch_count)))\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= ns\n",
    "    acc /= ns\n",
    "#     print (ns)\n",
    "#     print (n_sample)\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0,
     15.0,
     37.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    return yval\n",
    "\n",
    "def train(label):\n",
    "    print('\\nTraining')\n",
    "    n_sample = X_train.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 50\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_sample, start+batch_size)\n",
    "            sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                           env.y: y_train[start:end],\n",
    "                                           env.training: True})\n",
    "        if(epoch%5 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            print(\"saving model \" + model_label)\n",
    "            save_model(model_label)\n",
    "            \n",
    "    save_model(label)\n",
    "\n",
    "def train_again(X, y, epochs):\n",
    "    #Not making batches, do that if size > 128\n",
    "    for i in range(X.shape[0]):\n",
    "        for e in range(epochs):\n",
    "            sess.run(env.optim, feed_dict={env.x: [X[i]],\n",
    "                                           env.y: [y[i]],\n",
    "                                           env.training: True})\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_adv_wrt_class(X, Y, label):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    pred = np.argmax(Y,axis=1)\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    x_adv_shape = list(X.shape)[1:]\n",
    "    x_adv_shape = np.append(np.append(n_sample,n_classes),x_adv_shape)\n",
    "    X_adv = np.empty(x_adv_shape)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp = sess.run(env.x_adv_wrt_class, feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        tmp[pred[start]] = X[start]\n",
    "        X_adv[start:end] = tmp\n",
    "    print('\\nSaving adversarial')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    save_as_txt(get_flip_path(label), X_adv)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_adv(X, Y, label):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    X_adv = np.empty_like(X)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp, all_flipped = sess.run([env.x_adv, env.all_flipped], feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        X_adv[start:end] = tmp\n",
    "    print('\\nSaving adversarial')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    save_as_txt(get_flip_path(label), X_adv)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "method = 2\n",
    "n = 1000\n",
    "epochs = 200\n",
    "label=\"cifar_with_cnn\"\n",
    "lyrs= 4\n",
    "\n",
    "# train(label)\n",
    "restore_model(label + str(epochs))\n",
    "# _evaluate(X_train, y_train, env)\n",
    "\n",
    "for i in range(lyrs):\n",
    "    plot_all_data_graph(method,epochs,n,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.plot([1,2,3],[1,2,3])\n",
    "ax2 = plt.subplot(gs[0,1])\n",
    "ax3 = plt.subplot(gs[1, 0])\n",
    "ax4 = plt.subplot(gs[1,1])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 10.5)\n",
    "plt.savefig('test.png', dpi=80)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

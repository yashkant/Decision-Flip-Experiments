{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/yash/Desktop/tensorflow-adversarial/tf_example')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import ModeKeys, Estimator\n",
    "import _pickle as pickle\n",
    "from scipy.misc import imread\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from fgsm_cifar import fgsm\n",
    "from fgsm_cifar_wrt_class import fgsm_wrt_class\n",
    "import mnist\n",
    "import sys  \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "img_rows = 32\n",
    "img_cols = 32\n",
    "img_chas = 3\n",
    "input_shape = (img_rows, img_cols, img_chas)\n",
    "n_classes = 10\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f,encoding='latin1')\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def find_l2(X_test, X_adv):\n",
    "    a=X_test.reshape(-1,32*32*3)\n",
    "    b=X_adv.reshape(-1,32*32*3)\n",
    "    l2_unsquared = np.sum(np.square(a-b),axis=1)\n",
    "    return l2_unsquared\n",
    "\n",
    "def find_l2_batch(X_test, X_adv):\n",
    "    ans = np.zeros([X_test.shape[0],n_classes], dtype = np.float32)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for j in range(n_classes):\n",
    "            ans[i][j] = find_l2(X_test[i], X_adv[i][j])\n",
    "    return ans\n",
    "\n",
    "\n",
    "# m2 is the grouped flipping\n",
    "# m1 is the single flipping\n",
    "#This method returns the distance of each predictions from repective test points calculated by m1 and m2 resp. \n",
    "def find_m1_m2(X_test,X_adv_one,X_adv_test):\n",
    "    dist_adv_m1 = find_l2(X_test, X_adv_one)\n",
    "    b = find_l2_batch(X_test, X_adv_test)\n",
    "    dist_adv_m2 = np.partition(b,axis=1,kth=1)[:,1]\n",
    "    return np.sqrt(dist_adv_m1), np.sqrt(dist_adv_m2)\n",
    "\n",
    "# Give this function X_adv_test it gives you the points corresponding to\n",
    "# each example having min dists and their indices \n",
    "\n",
    "\n",
    "def give_m2_ans(X_test, X_adv_test,cls= -1):\n",
    "    if(cls == -1):\n",
    "        dists = find_l2_batch(X_test, X_adv_test)\n",
    "        second_min_indices = np.partition(dists, axis=1, kth=1)[:,1]\n",
    "        for i in range(X_test.shape[0]):\n",
    "            second_min_indices[i] = (np.where(second_min_indices[i] == dists[i])[0][0])\n",
    "        ans = np.empty([X_adv_test.shape[0], img_rows, img_cols, img_chas])\n",
    "        for i in range(ans.shape[0]):\n",
    "            ans[i] = X_adv_test[i][second_min_indices[i].astype(int)]\n",
    "        return second_min_indices, ans\n",
    "    else:\n",
    "        return 0, get_flipped_class(X_adv_test,cls)\n",
    "\n",
    "def remove_zeroes(X):\n",
    "    indices = np.where(X == 0)[0]\n",
    "    return np.delete(X,indices)\n",
    "\n",
    "def get_class(X,Y,cls):\n",
    "    Y=np.argmax(Y, axis=1)\n",
    "    indices = np.where(Y==cls)\n",
    "    return X[indices], Y[indices]\n",
    "\n",
    "def make_label(i,m,e,n,r):\n",
    "    if(r == False):\n",
    "        return i + \"_m\" + str(m) + \"_e\" + str(e) + \"_n\" + str(n)\n",
    "    else:\n",
    "        base = i + \"_m\" + str(m) + \"_e\" + str(e) + \"_n\" + str(n) + \"_r\"\n",
    "        lrn = base + \"normal\"\n",
    "        return base, lrn\n",
    "\n",
    "def get_flipped_class(X_adv,cls):\n",
    "    return X_adv[:,cls]\n",
    "    \n",
    "def make_data(n):\n",
    "    X_test_sub = X_test[:n]\n",
    "    X_train_sub = X_train[:n]\n",
    "    y_train_sub = sess.run(env.ybar, feed_dict={env.x: X_train_sub,env.training: False})\n",
    "    y_train_sub = _to_categorical(np.argmax(y_train_sub, axis=1), n_classes)\n",
    "    y_test_sub = sess.run(env.ybar, feed_dict={env.x:X_test_sub ,env.training: False})\n",
    "    y_test_sub = _to_categorical(np.argmax(y_test_sub, axis=1), n_classes)\n",
    "    \n",
    "    return X_test_sub, y_test_sub, X_train_sub, y_train_sub\n",
    "\n",
    "def get_flip_path(l):\n",
    "    return 'data/cifar/' + l + '.npz'\n",
    "\n",
    "def random_normal_func(X, n, save, lr, lrn):\n",
    "    X=X.reshape(-1,img_rows*img_cols*img_chas)\n",
    "    mean, std = np.mean(X, axis=0), np.std(X,axis=0)\n",
    "    randomX = np.zeros([n,X[0].size])\n",
    "    for i in range(X[0].size):\n",
    "        randomX[:,i] = np.random.normal(mean[i],std[i],n)\n",
    "    randomX = randomX.reshape(-1,img_rows,img_cols,img_chas)\n",
    "    X_random_normal = randomX\n",
    "    ans = sess.run(env.ybar, feed_dict={env.x: randomX,env.training: False})\n",
    "    y_random_normal = _to_categorical(np.argmax(ans,axis=1), n_classes)\n",
    "    X_random = np.random.rand(n,img_rows,img_cols,img_chas)\n",
    "    y_random = sess.run(env.ybar, feed_dict={env.x: X_random,env.training: False})\n",
    "    y_random = _to_categorical(np.argmax(y_random, axis=1), n_classes)\n",
    "    \n",
    "    if(save):\n",
    "        np.savez(get_flip_path(lr), Xr=X_random)\n",
    "        np.savez(get_flip_path(lrn), Xrn=X_random_normal)\n",
    "    \n",
    "    return X_random, y_random, X_random_normal, y_random_normal  \n",
    "\n",
    "def run_flip(method, epochs, n, cls=-1):\n",
    "    test_label = make_label(\"test\", method, epochs,n, False)\n",
    "    train_label = make_label(\"train\", method, epochs,n, False)\n",
    "    random_label = make_label(\"random\", method, epochs,n, False)\n",
    "    random_normal_label = make_label(\"random_normal\", method, epochs,n, False)\n",
    "    data_label_random, data_label_random_normal = make_label(\"_\", method, epochs,n, True)\n",
    "    \n",
    "    X_test_sub, y_test_sub, X_train_sub, y_train_sub = make_data(n)\n",
    "    X_random, y_random, X_random_normal, y_random_normal = random_normal_func(X_train,n, True, \n",
    "                                                                              data_label_random, data_label_random_normal)\n",
    "    if(cls < -2 or cls > 9):\n",
    "        print(\"Invalid Params\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if(method==1):\n",
    "        X_flip_test = create_adv(X_test_sub, y_test_sub, test_label)\n",
    "        X_flip_train = create_adv(X_train_sub, y_train_sub, train_label)\n",
    "        X_flip_random = create_adv(X_random, y_random, random_label)\n",
    "        X_flip_random_normal = create_adv(X_random_normal, y_random_normal, random_normal_label)\n",
    "    \n",
    "    if(method==2):\n",
    "        X_flip_per_class_test = create_adv_wrt_class(X_test_sub, y_test_sub, test_label)\n",
    "        X_flip_per_class_train = create_adv_wrt_class(X_train_sub, y_train_sub, train_label)\n",
    "        X_flip_per_class_random = create_adv_wrt_class(X_random, y_random, random_label)\n",
    "        X_flip_per_class_random_normal = create_adv_wrt_class(X_random_normal, y_random_normal, random_normal_label)\n",
    "        \n",
    "        _, X_flip_test = give_m2_ans(X_test_sub, X_flip_per_class_test, cls)\n",
    "        _, X_flip_train = give_m2_ans(X_train_sub, X_flip_per_class_train, cls)\n",
    "        _, X_flip_random = give_m2_ans(X_random,X_flip_per_class_random, cls)\n",
    "        _, X_flip_random_normal = give_m2_ans(X_random_normal, X_flip_per_class_random_normal,cls)\n",
    "    \n",
    "#     a = _predict(X_flip_test, env)\n",
    "#     print(np.argmax(a,axis=1))\n",
    "    \n",
    "    l2_test = find_l2(X_flip_test,X_test_sub)\n",
    "    l2_train = find_l2(X_flip_train, X_train_sub)\n",
    "    l2_random = find_l2(X_flip_random,X_random)\n",
    "    l2_random_normal = find_l2(X_flip_random_normal,X_random_normal)\n",
    "\n",
    "    return l2_test, l2_train, l2_random, l2_random_normal\n",
    "\n",
    "\n",
    "def restore_random_data(lr, lrn):\n",
    "    Xr = np.load(get_flip_path(lr))['Xr']\n",
    "    Xrn = np.load(get_flip_path(lrn))['Xrn']\n",
    "    y_random_normal = sess.run(env.ybar, feed_dict={env.x: Xrn,env.training: False})\n",
    "    y_random_normal = _to_categorical(np.argmax(y_random_normal,axis=1), n_classes)\n",
    "    y_random = sess.run(env.ybar, feed_dict={env.x: Xr,env.training: False})\n",
    "    y_random = _to_categorical(np.argmax(y_random, axis=1), n_classes)\n",
    "    \n",
    "    return Xr, y_random, Xrn, y_random_normal\n",
    "\n",
    "\n",
    "def restore_flip(method, epochs, n):\n",
    "    test_label = make_label(\"test\", method, epochs,n, False)\n",
    "    train_label = make_label(\"train\", method, epochs,n, False)\n",
    "    random_label = make_label(\"random\", method, epochs,n, False)\n",
    "    random_normal_label = make_label(\"random_normal\", method, epochs,n, False)\n",
    "    data_label_random, data_label_random_normal = make_label(\"_\", method, epochs,n, True)\n",
    "    \n",
    "    X_test_sub, y_test_sub, X_train_sub, y_train_sub = make_data(n)\n",
    "    X_random, y_random, X_random_normal, y_random_normal = restore_random_data(data_label_random, data_label_random_normal)\n",
    "    \n",
    "    if(method==1):\n",
    "        X_flip_test =  np.load(get_flip_path(test_label))['X_adv']\n",
    "        X_flip_train = np.load(get_flip_path(train_label))['X_adv']\n",
    "        X_flip_random = np.load(get_flip_path(random_label))['X_adv']\n",
    "        X_flip_random_normal = np.load(get_flip_path(random_normal_label))['X_adv']\n",
    "    \n",
    "    if(method==2):\n",
    "        X_flip_per_class_test = np.load(get_flip_path(test_label))['X_adv']\n",
    "        X_flip_per_class_train = np.load(get_flip_path(train_label))['X_adv']\n",
    "        X_flip_per_class_random = np.load(get_flip_path(random_label))['X_adv']\n",
    "        X_flip_per_class_random_normal = np.load(get_flip_path(random_normal_label))['X_adv']\n",
    "        \n",
    "        _, X_flip_test = give_m2_ans(X_test_sub, X_flip_per_class_test )\n",
    "        _, X_flip_train = give_m2_ans(X_train_sub, X_flip_per_class_train)\n",
    "        _, X_flip_random = give_m2_ans(X_random,X_flip_per_class_random)\n",
    "        _, X_flip_random_normal = give_m2_ans(X_random_normal, X_flip_per_class_random_normal)\n",
    "    \n",
    "    l2_test = find_l2(X_flip_test,X_test_sub)\n",
    "    l2_train = find_l2(X_flip_train, X_train_sub)\n",
    "    l2_random = find_l2(X_flip_random,X_random)\n",
    "    l2_random_normal = find_l2(X_flip_random_normal,X_random_normal)\n",
    "\n",
    "    return l2_test, l2_train, l2_random, l2_random_normal\n",
    "\n",
    "\n",
    "def count_clear(l2_test, l2_train, l2_random, l2_random_normal):\n",
    "    nz_test = np.count_nonzero(l2_test)\n",
    "    nz_train = np.count_nonzero(l2_train)\n",
    "    nz_random = np.count_nonzero(l2_random)\n",
    "    nz_random_normal = np.count_nonzero(l2_random_normal)\n",
    "\n",
    "    print ('\\n test: ' + str(nz_test))\n",
    "    print ('train: ' + str(nz_train))\n",
    "    print ('random: ' + str(nz_random))\n",
    "    print ('random normal: ' + str(nz_random_normal))\n",
    "\n",
    "    l2_test = remove_zeroes(l2_test)\n",
    "    l2_random = remove_zeroes(l2_random)\n",
    "    l2_random_normal = remove_zeroes(l2_random_normal)\n",
    "    l2_train = remove_zeroes(l2_train)\n",
    "\n",
    "    min_no = min(nz_test, nz_train)\n",
    "    l2_train = np.sqrt(l2_train[:min_no])\n",
    "    l2_test = np.sqrt(l2_test[:min_no])\n",
    "    l2_random = np.sqrt(l2_random[:min_no])\n",
    "    l2_random_normal = np.sqrt(l2_random_normal[:min_no])\n",
    "\n",
    "    return l2_test, l2_train, l2_random, l2_random_normal\n",
    "\n",
    "def plot_data_graph(l2_test, l2_train, l2_random, l2_random_normal, n):\n",
    "    %matplotlib inline\n",
    "    t = np.arange(1,n+1, 1)\n",
    "    plt.plot(t, l2_test[:n], 'r--', t, l2_train[:n],'b--', t, l2_random[:n], 'y--', l2_random_normal[:n], 'k--')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Train Data')\n",
    "    red_patch = mpatches.Patch(color='red', label='Test Data')\n",
    "    yellow_patch = mpatches.Patch(color='yellow', label='Random Data')\n",
    "    black_patch = mpatches.Patch(color='black', label='Random Normal Data')\n",
    "    plt.legend(handles=[blue_patch, red_patch, yellow_patch, black_patch])\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "def plot_data_hist(l2,n,title):\n",
    "    %matplotlib inline\n",
    "    plt.hist(l2,n)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR10\n",
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading CIFAR10')\n",
    "ab=sys.getdefaultencoding()\n",
    "print(ab)\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train.reshape(-1, img_rows, img_cols, img_chas)\n",
    "X_test = X_test.reshape(-1, img_rows, img_cols, img_chas)\n",
    "\n",
    "# X_train=X_train[:100]\n",
    "# y_train=y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shuffling training data\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding, basically creates hte si\n",
    "def _to_categorical(x, n_classes):\n",
    "    x = np.array(x, dtype=int).ravel()\n",
    "    n = x.shape[0]\n",
    "    ret = np.zeros((n, n_classes))\n",
    "    ret[np.arange(n), x] = 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "y_train = _to_categorical(y_train, n_classes)\n",
    "y_test = _to_categorical(y_test, n_classes)\n",
    "print('\\nShuffling training data')\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "# split training/validation dataset\n",
    "validation_split = 0.1\n",
    "n_train = int(X_train.shape[0]*(1-validation_split))\n",
    "X_valid = X_train[n_train:]\n",
    "X_train = X_train[:n_train]\n",
    "y_valid = y_train[n_train:]\n",
    "y_train = y_train[:n_train]\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "env = Dummy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, logits=False, training=False):\n",
    "    conv0 = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(pool0, filters=64,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "   \n",
    " \n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(pool1, filters=128,\n",
    "                             kernel_size=[1,1], padding='same',\n",
    "                             name='conv2', activation=tf.nn.relu)\n",
    "    \n",
    "\n",
    "    \n",
    "    flat = tf.reshape(conv2, [-1, 8*8*128], name='flatten')\n",
    "    \n",
    "    dense1 = tf.layers.dense(flat, units= 1024, activation=tf.nn.relu,\n",
    "                            name='dense1')\n",
    "    \n",
    "    dense2 = tf.layers.dense(dense1, units=128, activation=tf.nn.relu,\n",
    "                            name='dense2')\n",
    "    logits_ = tf.layers.dense(dense2, units=10, name='logits') #removed dropout\n",
    "    \n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "    \n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model'):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,\n",
    "                                        img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True,\n",
    "                             training=env.training)\n",
    "\n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    env.count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(env.count, name='acc')\n",
    "\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
    "                                                   logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    env.optim = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08,).minimize(env.loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model', reuse=True):\n",
    "    for i in range(n_classes):\n",
    "        if(i==0):\n",
    "            env.x_adv_wrt_class = (fgsm_wrt_class(model, env.x, i, step_size=.05, bbox_semi_side=10))\n",
    "        else:\n",
    "            x = (fgsm_wrt_class(model, env.x, i, step_size=.05, bbox_semi_side=10))\n",
    "            env.x_adv_wrt_class = tf.concat([env.x_adv_wrt_class, x],axis=0)\n",
    "    env.x_adv, env.all_flipped = fgsm(model, env.x, step_size=.05, bbox_semi_side=10) #epochs is redundant now!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,  './models/cifar/' + label)\n",
    "    \n",
    "def restore_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/cifar/' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_count, batch_acc = sess.run(\n",
    "            [env.loss, env.count, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        print('batch count: {0}'.format(np.sum(batch_count)))\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= ns\n",
    "    acc /= ns\n",
    "#     print (ns)\n",
    "#     print (n_sample)\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    return yval\n",
    "\n",
    "def train(label):\n",
    "    print('\\nTraining')\n",
    "    n_sample = X_train.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 50\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_sample, start+batch_size)\n",
    "            sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                           env.y: y_train[start:end],\n",
    "                                           env.training: True})\n",
    "        if(epoch%5 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            print(\"saving model \" + model_label)\n",
    "            save_model(model_label)\n",
    "            \n",
    "    save_model(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adv_wrt_class(X, Y, label):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    pred = np.argmax(Y,axis=1)\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    x_adv_shape = list(X.shape)[1:]\n",
    "    x_adv_shape = np.append(np.append(n_sample,n_classes),x_adv_shape)\n",
    "    X_adv = np.empty(x_adv_shape)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp = sess.run(env.x_adv_wrt_class, feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        tmp[pred[start]] = X[start]\n",
    "        X_adv[start:end] = tmp\n",
    "    print('\\nSaving adversarial')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    np.savez(get_flip_path(label), X_adv = X_adv)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adv(X, Y, label):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    X_adv = np.empty_like(X)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp, all_flipped = sess.run([env.x_adv, env.all_flipped], feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        X_adv[start:end] = tmp\n",
    "    print('\\nSaving adversarial')\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    np.savez(get_flip_path(label), X_adv = X_adv)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/cifar/cifar_with_cnn50\n",
      "\n",
      "Crafting adversarial\n",
      " batch 73/1000\r"
     ]
    }
   ],
   "source": [
    "method = 2\n",
    "n = 1000\n",
    "epochs = 50\n",
    "label=\"cifar_with_cnn\"\n",
    "cls = -1\n",
    "\n",
    "# train(label)\n",
    "restore_model(label + str(epochs))\n",
    "# _evaluate(X_train, y_train, env)\n",
    "\n",
    "l2_test, l2_train, l2_random, l2_random_normal = run_flip(method,epochs,n, cls)\n",
    "# l2_test, l2_train, l2_random, l2_random_normal = restore_flip(method,epochs,n)\n",
    "\n",
    "l2_test, l2_train, l2_random, l2_random_normal = count_clear(l2_test, l2_train, l2_random, l2_random_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data_graph(l2_test, l2_train, l2_random, l2_random_normal, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data_graph(l2_test, l2_train, l2_random, l2_random_normal, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"L2 distance of test data\"\n",
    "plot_data_hist(l2_test,n,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"L2 distance of train data\"\n",
    "plot_data_hist(l2_train,n,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"L2 distance of Random data\"\n",
    "plot_data_hist(l2_random,n,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"L2 distance of random normal data\"\n",
    "plot_data_hist(l2_random_normal,n,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.load('data/cifar/' + test_m2 + '.npy')\n",
    "# b = np.load('data/cifar/' + train_m2 + '.npy')\n",
    "# c = np.load('data/cifar/' + random_m2 + '.npy')\n",
    "# d = np.load('data/cifar/' + random_normal_m2 + '.npy')\n",
    "\n",
    "# _, X_adv_test_m2 = give_m2_ans(X_test_sub, a )\n",
    "# _, X_adv_train_m2 = give_m2_ans(X_train_sub, b )\n",
    "# _, X_adv_random_m2 = give_m2_ans(X_random, c)\n",
    "# _, X_adv_random_normal_m2 = give_m2_ans(X_random_normal, d)\n",
    "\n",
    "# a = np.load('data/cifar/' + test_m2 + '.npy')\n",
    "# b = np.load('data/cifar/' + train_m2 + '.npy')\n",
    "# c = np.load('data/cifar/' + random_m2 + '.npy')\n",
    "# d = np.load('data/cifar/' + random_normal_m2 + '.npy')\n",
    "\n",
    "# _, X_adv_test_m2 = give_m2_ans(X_test_sub, a )\n",
    "# _, X_adv_train_m2 = give_m2_ans(X_train_sub, b )\n",
    "\n",
    "# l2_test_m2 = find_l2(X_adv_test_m2,X_test_sub)\n",
    "# l2_train_m2 = find_l2(X_adv_train_m2, X_train_sub)\n",
    "# l2_random_m2 = find_l2(X_adv_random_m2,X_random)\n",
    "# l2_random_normal_m2 = find_l2(X_adv_random_normal_m2,X_random_normal)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir('/home/yash/Documents/Decision-Flip-Experiments')\n",
    "from scipy.misc import imread\n",
    "import matplotlib.patches as mpatches\n",
    "from models import *\n",
    "from plotter import *\n",
    "from saveloader import *\n",
    "from fgsm_cifar import fgsm\n",
    "from fgsm_cifar_wrt_class_flipstop import fgsm_wrt_class\n",
    "from helper import *\n",
    "from helper import _to_categorical\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "sd = 'shape_dict'\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_valid, y_valid = load_data(os)\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "\n",
    "env = Dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     7,
     10,
     12,
     15,
     17,
     74,
     81
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model'):\n",
    "    \n",
    "    #target-model\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,  img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "    conv0 = tf.layers.conv2d(env.x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    conv1 = tf.layers.conv2d(pool0, filters=64,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=128,\n",
    "                             kernel_size=[1, 1], padding='same',\n",
    "                             name='conv2', activation=tf.nn.relu)\n",
    "    flat = tf.reshape(conv2, [-1, 8 * 8 * 128], name='flatten')\n",
    "    logits = tf.layers.dense(flat, units=10, name='logits')\n",
    "\n",
    "    env.ybar = tf.nn.softmax(logits, name='ybar')\n",
    "    \n",
    "    #finding accuracy \n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    env.count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(env.count, name='acc')\n",
    "    \n",
    "    #finding loss \n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=env.y,logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "    \n",
    "    #finding activations\n",
    "    env.activations = [tf.reshape(env.x, [-1]), tf.reshape(pool0, [-1]), tf.reshape(pool1, [-1]), \n",
    "                      tf.reshape(flat, [-1]), tf.reshape(logits, [-1])]\n",
    "    \n",
    "    #finding activation gradients\n",
    "    grads_x = tf.reshape(tf.gradients(env.loss, env.x), [32*32*3]) #32x32x3\n",
    "    grads_xl1 = tf.reshape(tf.gradients(env.loss, pool0), [16*16*32]) #16x16x32\n",
    "    grads_xl2 = tf.reshape(tf.gradients(env.loss, pool1), [ 8*8*64]) #8x8x64\n",
    "    grads_xl3 = tf.reshape(tf.gradients(env.loss, flat), [ 8192]) #8192\n",
    "    grads_y = tf.reshape(tf.gradients(env.loss, logits), [ 10]) #10\n",
    "    \n",
    "    # [[32,32,3], [16,16,32], [8,8,64], 8192, 10]\n",
    "    env.grads_activations = [grads_x, grads_xl1, grads_xl2, grads_xl3, grads_y]\n",
    "    \n",
    "    #finding layer gradients    \n",
    "    l1_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv0.name)[0] + '/kernel:0')\n",
    "    grads_l1 = tf.reshape(tf.gradients(env.loss, l1_vars), [ 27*32])\n",
    "    l2_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv1.name)[0] + '/kernel:0')\n",
    "    grads_l2 = tf.reshape(tf.gradients(env.loss, l2_vars), [ 9*32*64])\n",
    "    l3_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv2.name)[0] + '/kernel:0')\n",
    "    grads_l3 = tf.reshape(tf.gradients(env.loss, l3_vars), [ 64*128])\n",
    "    l4_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(logits.name)[0] + '/kernel:0')\n",
    "    grads_l4 = tf.reshape(tf.gradients(env.loss, l4_vars), [ 8192*10])\n",
    "    \n",
    "    # [[3,3,3,32], [3,3,32,64], [1,1,64,128], [8192,10]]\n",
    "    env.grads_model = [grads_l1, grads_l2, grads_l3, grads_l4]\n",
    "    \n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    env.optim = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08,).minimize(env.loss)\n",
    "\n",
    "\n",
    "with tf.variable_scope('sub_model_1'):\n",
    "    shape_m1 = [32*32*3, 20, 2]\n",
    "    env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1, env.acc_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_2'):\n",
    "#     shape_m1 = [len(env.grads_activations[1]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# with tf.variable_scope('sub_model_3'):\n",
    "#     shape_m1 = [len(env.grads_activations[2]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "    \n",
    "\n",
    "# with tf.variable_scope('sub_model_4'):\n",
    "#     shape_m1 = [len(env.grads_activations[3]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_5'):\n",
    "#     shape_m1 = [len(env.grads_activations[4]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_6'):\n",
    "#     shape_m1 = [len(env.grads_activations[5]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_7'):\n",
    "#     shape_m1 = [len(env.grads_activations[6]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_8'):\n",
    "#     shape_m1 = [len(env.grads_activations[7]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_9'):\n",
    "#     shape_m1 = [len(env.grads_activations[8]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "# with tf.variable_scope('sub_model_10'):\n",
    "#     shape_m1 = [len(env.grads_activations[9]), 20, 1]\n",
    "#     env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1 = sub_attack_model(shape_m1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,  './models/cifar/' + label)\n",
    "    \n",
    "\n",
    "def restore_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/cifar/' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     24
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_count, batch_acc = sess.run(\n",
    "            [env.loss, env.count, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= ns\n",
    "    acc /= ns\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc\n",
    "\n",
    "def _evaluate_submodel(i, X_data, y_data):\n",
    "    print('Evaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    \n",
    "    attr_x = \"x_m\" + str(i)\n",
    "    attr_y = \"y_m\" + str(i)\n",
    "    attr_optim = \"optimizer_m\" + str(i)\n",
    "    attr_acc = \"acc_m\" + str(i)\n",
    "    attr_preds = \"preds_m\" + str(i)\n",
    "    \n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_acc = sess.run(getattr(env, attr_acc),\n",
    "            feed_dict={getattr(env, attr_x): X_data[start:end],\n",
    "                       getattr(env, attr_y): y_data[start:end],\n",
    "                       env.training: False})\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    acc /= ns\n",
    "    print('acc: {0:.4f}'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     36,
     48
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    return yval\n",
    "\n",
    "def train(label = 'generalized_model', n_epoch = 20, batch_size = 128):\n",
    "    print('\\nTraining')\n",
    "    n_sample = X_train.shape[0]\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_sample, start+batch_size)\n",
    "            sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                           env.y: y_train[start:end],\n",
    "                                           env.training: True})\n",
    "        if((epoch+1)%10 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            _, train_acc = _evaluate(X_train, y_train, env)\n",
    "            print(\"Train Acc. :\" + str(train_acc))\n",
    "            save_model(model_label)\n",
    "            \n",
    "    save_model(label)\n",
    "\n",
    "def train_again(X, y, epochs):\n",
    "    #Not making batches, do that if size > 128\n",
    "    for i in range(X.shape[0]):\n",
    "        for e in range(epochs):\n",
    "            sess.run(env.optim, feed_dict={env.x: [X[i]],\n",
    "                                           env.y: [y[i]],\n",
    "                                           env.training: True})\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "#send i+1\n",
    "def train_submodel(i, X, y, n_epoch = 20, batch_size = 128):\n",
    "    print('Training ' + str(i) + 'th model.')\n",
    "    n_samples = X.shape[0]\n",
    "    n_batch = int(np.ceil(n_samples/batch_size))\n",
    "    \n",
    "    attr_x = \"x_m\" + str(i)\n",
    "    attr_y = \"y_m\" + str(i)\n",
    "    attr_optim = \"optimizer_m\" + str(i)\n",
    "    attr_acc = \"acc_m\" + str(i)\n",
    "    attr_preds = \"preds_m\" + str(i)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        \n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_samples, start+batch_size)\n",
    "            sess.run(getattr(env, attr_optim), feed_dict={getattr(env, attr_x): X[start:end],\n",
    "                                           getattr(env, attr_y): y[start:end],\n",
    "                                           env.training: True})\n",
    "        if((epoch+1)%10 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            train_acc = _evaluate_submodel(i, X, y)\n",
    "#             save_model(model_label)\n",
    "            \n",
    "#     save_model(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     25,
     39,
     43,
     46,
     49,
     55,
     63
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_flip_wrt_class(X, Y, label = None):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    pred = np.argmax(Y,axis=1)\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    x_adv_shape = list(X.shape)[1:]\n",
    "    x_adv_shape = np.append(np.append(n_sample,n_classes),x_adv_shape)\n",
    "    X_adv = np.empty(x_adv_shape)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp = sess.run(env.x_adv_wrt_class, feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        tmp[pred[start]] = X[start]\n",
    "        X_adv[start:end] = tmp\n",
    "    if(label != None):\n",
    "        print('\\nSaving adversarial')\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        save_as_txt(get_flip_path(label), X_adv)\n",
    "    return X_adv    \n",
    "\n",
    "def get_signals(X,y):\n",
    "    activation_grads = []\n",
    "    model_grads = []\n",
    "    activations = []\n",
    "    for i in range(len(X)):\n",
    "        acts, ag, mg = sess.run([env. activations, env.grads_activations, env.grads_model], \n",
    "                          feed_dict={env.x: [X[i]], env.y: [y[i]]})\n",
    "        activations.append(acts)\n",
    "        activation_grads.append(ag)\n",
    "        model_grads.append(mg)\n",
    "        \n",
    "    return np.array(activations), np.array(activation_grads), np.array(model_grads)\n",
    "\n",
    "# create a batchwise call in build_submodelD if required! \n",
    "def build_submodelX(X,y):\n",
    "    activations, activation_grads, model_grads = get_signals(X, y)\n",
    "    aX, agX, mgX = [],[],[]\n",
    "    \n",
    "    for i in range (activations.shape[1]):\n",
    "        aX.append([item[i] for item in activations])\n",
    "    \n",
    "    for i in range (activation_grads.shape[1]):\n",
    "        agX.append([item[i] for item in activation_grads])\n",
    "    \n",
    "    for i in range (model_grads.shape[1]):\n",
    "        mgX.append([item[i] for item in model_grads])\n",
    "    \n",
    "    return aX+ agX+ mgX\n",
    "\n",
    "def merge(a,b):\n",
    "    if(len(a) == 0):\n",
    "        return b\n",
    "    for i in range(len(a)):\n",
    "        a[i] = a[i] + b[i]\n",
    "    return a\n",
    "    \n",
    "\n",
    "def build_submodelD():\n",
    "    #taking 5000 test and 5000 train images\n",
    "    n = 2000\n",
    "    sy = np.array([[0,1]]*n + [[1,0]]*n, np.float32)\n",
    "    X = np.array(list(X_test[:n]) + list(X_train[:n]))\n",
    "    y = np.array(list(y_test[:n]) + list(y_test[:n]))\n",
    "    batch_size = 64\n",
    "    n_samples = X.shape[0]\n",
    "    n_batch = int(np.ceil(n_samples/batch_size))\n",
    "    \n",
    "    sX=[]\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_samples, start+batch_size)\n",
    "        tsX = build_submodelX(X[start:end], y[start:end])\n",
    "        sX = merge(sX, tsX)\n",
    "    return sX, sy\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train(n_epoch=1)\n",
    "label = \"my-model\"\n",
    "# save_model(label)\n",
    "restore_model(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sX, sy = build_submodelD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sX)):\n",
    "    print(len(sx[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #training m1\n",
    "\n",
    "imgs = np.reshape(imgs, [-1, 3072])\n",
    "# feed_dict={env.x_m1: imgs, env.y_m1: imgs_y, env.training: True}\n",
    "# sess.run(getattr(env, 'optimizer_m1'), feed_dict)\n",
    "# print(sess.run(getattr(env, 'acc_m1'), feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_submodel(1, imgs, imgs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1])\n",
    "a = np.append(a,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "en = [activations[0], activation_grads[0], model_grads[0]]\n",
    "print (len(en))\n",
    "print (len(en[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "activation_grads = [item[0] for item in ag] # gets the first item in each sublist.\n",
    "\n",
    "print(len(activation_grads))\n",
    "print(len(activation_grads[0]))\n",
    "print(len(activation_grads[0][0]))\n",
    "print(len(activation_grads[0][0][0]))\n",
    "# print(len(activation_grads[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def main_func(X,y,model_label):\n",
    "#     restore_model(model_label)\n",
    "#     Xflip = create_flip_wrt_class(X,y)\n",
    "#     return Xflip\n",
    "\n",
    "\n",
    "# n = 10\n",
    "# model_label=\"cifar_with_cnn\" + str(200)\n",
    "# X = X_train[:n]\n",
    "# y = y_train[:n]\n",
    "# X_flip = main_func(X,y,model_label)\n",
    "\n",
    "# flip_label = \"flipped_examples\"\n",
    "# save_as_txt(get_flip_path(flip_label), X_flip)\n",
    "# X_flip_load = load_from_txt(get_flip_path(flip_label))\n",
    "\n",
    "# # print(X_flip == X_flip_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "obj = Dummy()\n",
    "\n",
    "obj.hi = \"yash\"\n",
    "obj.bye = \"yash\"\n",
    "\n",
    "a = \"bye\"\n",
    "print(getattr(obj,a))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

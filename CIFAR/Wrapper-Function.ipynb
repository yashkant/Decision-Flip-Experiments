{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR10\n",
      "Loading pre-Shuffled training data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from math import sqrt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir('/home/yash/Documents/Decision-Flip-Experiments')\n",
    "from scipy.misc import imread\n",
    "import matplotlib.patches as mpatches\n",
    "from models import *\n",
    "from plotter import *\n",
    "from saveloader import *\n",
    "from fgsm_cifar import fgsm\n",
    "from fgsm_cifar_wrt_class_flipstop import fgsm_wrt_class\n",
    "from helper import *\n",
    "from helper import _to_categorical\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "sd = 'shape_dict'\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_valid, y_valid = load_data(os)\n",
    "\n",
    "class Dummy:\n",
    "    pass\n",
    "\n",
    "env = Dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     7,
     10,
     12,
     15,
     17,
     64,
     67,
     72,
     77,
     82,
     87,
     92,
     97,
     102,
     107,
     112,
     117,
     122,
     127,
     132
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model'):\n",
    "    \n",
    "    #target-model\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,  img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "    conv0 = tf.layers.conv2d(env.x, filters=32, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    conv1 = tf.layers.conv2d(pool0, filters=64,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=128,\n",
    "                             kernel_size=[1, 1], padding='same',\n",
    "                             name='conv2', activation=tf.nn.relu)\n",
    "    flat = tf.reshape(conv2, [-1, 8 * 8 * 128], name='flatten')\n",
    "    logits = tf.layers.dense(flat, units=10, name='logits')\n",
    "\n",
    "    env.ybar = tf.nn.softmax(logits, name='ybar')\n",
    "    \n",
    "    #finding accuracy \n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    env.count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(env.count, name='acc')\n",
    "    \n",
    "    #finding loss \n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=env.y,logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "    \n",
    "    #finding activations\n",
    "    env.activations = [tf.reshape(env.x, [-1]), tf.reshape(pool0, [-1]), tf.reshape(pool1, [-1]), \n",
    "                      tf.reshape(flat, [-1]), tf.reshape(logits, [-1])]\n",
    "    \n",
    "    #finding activation gradients\n",
    "    grads_x = tf.reshape(tf.gradients(env.loss, env.x), [32*32*3]) #32x32x3\n",
    "    grads_xl1 = tf.reshape(tf.gradients(env.loss, pool0), [16*16*32]) #16x16x32\n",
    "    grads_xl2 = tf.reshape(tf.gradients(env.loss, pool1), [ 8*8*64]) #8x8x64\n",
    "    grads_xl3 = tf.reshape(tf.gradients(env.loss, flat), [ 8192]) #8192\n",
    "    grads_y = tf.reshape(tf.gradients(env.loss, logits), [ 10]) #10\n",
    "    \n",
    "    # [[32,32,3], [16,16,32], [8,8,64], 8192, 10]\n",
    "    env.grads_activations = [grads_x, grads_xl1, grads_xl2, grads_xl3, grads_y]\n",
    "    \n",
    "    #finding layer gradients    \n",
    "    l1_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv0.name)[0] + '/kernel:0')\n",
    "    grads_l1 = tf.reshape(tf.gradients(env.loss, l1_vars), [ 27*32])\n",
    "    l2_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv1.name)[0] + '/kernel:0')\n",
    "    grads_l2 = tf.reshape(tf.gradients(env.loss, l2_vars), [ 9*32*64])\n",
    "    l3_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(conv2.name)[0] + '/kernel:0')\n",
    "    grads_l3 = tf.reshape(tf.gradients(env.loss, l3_vars), [ 64*128])\n",
    "    l4_vars = tf.get_default_graph().get_tensor_by_name(os.path.split(logits.name)[0] + '/kernel:0')\n",
    "    grads_l4 = tf.reshape(tf.gradients(env.loss, l4_vars), [ 8192*10])\n",
    "    \n",
    "    # [[3,3,3,32], [3,3,32,64], [1,1,64,128], [8192,10]]\n",
    "    env.grads_model = [grads_l1, grads_l2, grads_l3, grads_l4]\n",
    "    \n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    env.optim = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08,).minimize(env.loss)\n",
    "\n",
    "with tf.variable_scope('sub_model_1'):\n",
    "    inp = 3072\n",
    "    shape_m1 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m1, env.y_m1, env.optimizer_m1, env.preds_m1, env.acc_m1 = sub_attack_model(shape_m1)\n",
    "\n",
    "with tf.variable_scope('sub_model_2'):\n",
    "    inp = 8192\n",
    "    shape_m2 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m2, env.y_m2, env.optimizer_m2, env.preds_m2, env.acc_m2 = sub_attack_model(shape_m2)\n",
    "\n",
    "with tf.variable_scope('sub_model_3'):\n",
    "    inp = 4096\n",
    "    shape_m3 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m3, env.y_m3, env.optimizer_m3, env.preds_m3, env.acc_m3 = sub_attack_model(shape_m3)\n",
    "\n",
    "with tf.variable_scope('sub_model_4'):\n",
    "    inp = 8192\n",
    "    shape_m4 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m4, env.y_m4, env.optimizer_m4, env.preds_m4, env.acc_m4 = sub_attack_model(shape_m4)\n",
    "\n",
    "with tf.variable_scope('sub_model_5'):\n",
    "    inp = 10\n",
    "    shape_m5 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m5, env.y_m5, env.optimizer_m5, env.preds_m5, env.acc_m5 = sub_attack_model(shape_m5)\n",
    "\n",
    "with tf.variable_scope('sub_model_6'):\n",
    "    inp = 3072\n",
    "    shape_m6 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m6, env.y_m6, env.optimizer_m6, env.preds_m6, env.acc_m6 = sub_attack_model(shape_m6)\n",
    "\n",
    "with tf.variable_scope('sub_model_7'):\n",
    "    inp = 8192\n",
    "    shape_m7 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m7, env.y_m7, env.optimizer_m7, env.preds_m7, env.acc_m7 = sub_attack_model(shape_m7)\n",
    "\n",
    "with tf.variable_scope('sub_model_8'):\n",
    "    inp = 4096\n",
    "    shape_m8 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m8, env.y_m8, env.optimizer_m8, env.preds_m8, env.acc_m8 = sub_attack_model(shape_m8)\n",
    "\n",
    "with tf.variable_scope('sub_model_9'):\n",
    "    inp = 8192\n",
    "    shape_m9 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m9, env.y_m9, env.optimizer_m9, env.preds_m9, env.acc_m9 = sub_attack_model(shape_m9)\n",
    "\n",
    "with tf.variable_scope('sub_model_10'):\n",
    "    inp = 10\n",
    "    shape_m10 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m10, env.y_m10, env.optimizer_m10, env.preds_m10, env.acc_m10 = sub_attack_model(shape_m10)\n",
    "\n",
    "with tf.variable_scope('sub_model_11'):\n",
    "    inp = 864\n",
    "    shape_m11 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m11, env.y_m11, env.optimizer_m11, env.preds_m11, env.acc_m11 = sub_attack_model(shape_m11)\n",
    "\n",
    "with tf.variable_scope('sub_model_12'):\n",
    "    inp = 18432\n",
    "    shape_m12 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m12, env.y_m12, env.optimizer_m12, env.preds_m12, env.acc_m12 = sub_attack_model(shape_m12)\n",
    "\n",
    "with tf.variable_scope('sub_model_13'):\n",
    "    inp = 8192\n",
    "    shape_m13 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m13, env.y_m13, env.optimizer_m13, env.preds_m13, env.acc_m13 = sub_attack_model(shape_m13)\n",
    "\n",
    "with tf.variable_scope('sub_model_14'):\n",
    "    inp = 81920\n",
    "    shape_m14 = [inp, int(sqrt(inp)), 2]\n",
    "    env.x_m14, env.y_m14, env.optimizer_m14, env.preds_m14, env.acc_m14 = sub_attack_model(shape_m14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,  './models/cifar/' + label)\n",
    "    \n",
    "\n",
    "def restore_model(label):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/cifar/' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_count, batch_acc = sess.run(\n",
    "            [env.loss, env.count, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= ns\n",
    "    acc /= ns\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc\n",
    "\n",
    "def _evaluate_submodel(i, X_data, y_data):\n",
    "    print('Evaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    ns = 0\n",
    "    \n",
    "    attr_x = \"x_m\" + str(i)\n",
    "    attr_y = \"y_m\" + str(i)\n",
    "    attr_optim = \"optimizer_m\" + str(i)\n",
    "    attr_acc = \"acc_m\" + str(i)\n",
    "    attr_preds = \"preds_m\" + str(i)\n",
    "    \n",
    "    for ind in range(n_batch):\n",
    "#         print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_acc = sess.run(getattr(env, attr_acc),\n",
    "            feed_dict={getattr(env, attr_x): X_data[start:end],\n",
    "                       getattr(env, attr_y): y_data[start:end],\n",
    "                       env.training: False})\n",
    "        ns+=batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    acc /= ns\n",
    "    print('acc: {0:.4f}'.format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0,
     15,
     36
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    return yval\n",
    "\n",
    "def train(label = 'generalized_model', n_epoch = 20, batch_size = 128):\n",
    "    print('\\nTraining')\n",
    "    n_sample = X_train.shape[0]\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "        for ind in range(n_batch):\n",
    "            print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_sample, start+batch_size)\n",
    "            sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                           env.y: y_train[start:end],\n",
    "                                           env.training: True})\n",
    "        if((epoch+1)%10 == 0):\n",
    "            model_label = label+ '{0}'.format(epoch)\n",
    "            _, train_acc = _evaluate(X_train, y_train, env)\n",
    "            print(\"Train Acc. :\" + str(train_acc))\n",
    "            save_model(model_label)\n",
    "            \n",
    "    save_model(label)\n",
    "\n",
    "def train_again(X, y, epochs):\n",
    "    #Not making batches, do that if size > 128\n",
    "    for i in range(X.shape[0]):\n",
    "        for e in range(epochs):\n",
    "            sess.run(env.optim, feed_dict={env.x: [X[i]],\n",
    "                                           env.y: [y[i]],\n",
    "                                           env.training: True})\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "#send i+1\n",
    "def train_submodel(i, X, y, n_epoch = 100, batch_size = 128, disp_freq=10):\n",
    "    print('\\n Training ' + str(i) + 'th model.')\n",
    "    n_samples = X.shape[0]\n",
    "    n_batch = int(np.ceil(n_samples/batch_size))\n",
    "    \n",
    "    attr_x = \"x_m\" + str(i)\n",
    "    attr_y = \"y_m\" + str(i)\n",
    "    attr_optim = \"optimizer_m\" + str(i)\n",
    "    attr_acc = \"acc_m\" + str(i)\n",
    "    attr_preds = \"preds_m\" + str(i)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, n_epoch), end = '\\r')\n",
    "        \n",
    "        for ind in range(n_batch):\n",
    "#             print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "            start = ind*batch_size\n",
    "            end = min(n_samples, start+batch_size)\n",
    "            feed_dict = {getattr(env, attr_x): X[start:end], getattr(env, attr_y): y[start:end], env.training: True}\n",
    "            sess.run(getattr(env, attr_optim), feed_dict)\n",
    "            \n",
    "        if((epoch+1)%disp_freq == 0):\n",
    "            train_acc = _evaluate_submodel(i, X, y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     25,
     39,
     43,
     46,
     49,
     54,
     62
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_flip_wrt_class(X, Y, label = None):\n",
    "    print('\\nCrafting adversarial')\n",
    "    n_sample = X.shape[0]\n",
    "    pred = np.argmax(Y,axis=1)\n",
    "    batch_size = 1\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    n_epoch = 20\n",
    "    x_adv_shape = list(X.shape)[1:]\n",
    "    x_adv_shape = np.append(np.append(n_sample,n_classes),x_adv_shape)\n",
    "    X_adv = np.empty(x_adv_shape)\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        tmp = sess.run(env.x_adv_wrt_class, feed_dict={env.x: X[start:end],\n",
    "                                             env.y: Y[start:end],\n",
    "                                             env.training: False})\n",
    "        tmp[pred[start]] = X[start]\n",
    "        X_adv[start:end] = tmp\n",
    "    if(label != None):\n",
    "        print('\\nSaving adversarial')\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        save_as_txt(get_flip_path(label), X_adv)\n",
    "    return X_adv    \n",
    "\n",
    "def get_signals(X,y):\n",
    "    activation_grads = []\n",
    "    model_grads = []\n",
    "    activations = []\n",
    "    for i in range(len(X)):\n",
    "        acts, ag, mg = sess.run([env. activations, env.grads_activations, env.grads_model], \n",
    "                          feed_dict={env.x: [X[i]], env.y: [y[i]]})\n",
    "        activations.append(acts)\n",
    "        activation_grads.append(ag)\n",
    "        model_grads.append(mg)\n",
    "        \n",
    "    return np.array(activations), np.array(activation_grads), np.array(model_grads)\n",
    "\n",
    "# create a batchwise call in build_submodelD if required! \n",
    "def build_submodelX(X,y):\n",
    "    activations, activation_grads, model_grads = get_signals(X, y)\n",
    "    aX, agX, mgX = [],[],[]\n",
    "    \n",
    "    for i in range (activations.shape[1]):\n",
    "        aX.append([item[i] for item in activations])\n",
    "    \n",
    "    for i in range (activation_grads.shape[1]):\n",
    "        agX.append([item[i] for item in activation_grads])\n",
    "    \n",
    "    for i in range (model_grads.shape[1]):\n",
    "        mgX.append([item[i] for item in model_grads])\n",
    "    \n",
    "    return aX+ agX+ mgX\n",
    "\n",
    "def merge(a,b):\n",
    "    if(len(a) == 0):\n",
    "        return b\n",
    "    for i in range(len(a)):\n",
    "        a[i] = a[i] + b[i]\n",
    "    return a\n",
    "    \n",
    "\n",
    "def build_submodelD():\n",
    "    #taking 5000 test and 5000 train images\n",
    "    n = 1000\n",
    "    sy = np.array([[0,1]]*n + [[1,0]]*n, np.float32)\n",
    "    X = np.array(list(X_test[:n]) + list(X_train[:n]))\n",
    "    y = np.array(list(y_test[:n]) + list(y_test[:n]))\n",
    "    batch_size = 64\n",
    "    n_samples = X.shape[0]\n",
    "    n_batch = int(np.ceil(n_samples/batch_size))\n",
    "    \n",
    "    sX=[]\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_samples, start+batch_size)\n",
    "        tsX = build_submodelX(X[start:end], y[start:end])\n",
    "        sX = merge(sX, tsX)\n",
    "    return sX, sy\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/cifar/my-model\n",
      " batch 32/32\n",
      " Training 1th model.\n",
      "Evaluating0\n",
      "acc: 0.51176\n",
      "Evaluating0\n",
      "acc: 0.51176\n",
      "\n",
      " Training 2th model.\n",
      "Evaluating0\n",
      "acc: 0.51176\n",
      "Evaluating0\n",
      "acc: 0.51176\n",
      "\n",
      " Training 3th model.\n",
      "Evaluating0\n",
      "acc: 0.51176\n",
      "Epoch 18/20\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4aadc8489266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_submodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7bfe73396b19>\u001b[0m in \u001b[0;36mtrain_submodel\u001b[0;34m(i, X, y, n_epoch, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train(n_epoch=1)\n",
    "label = \"my-model\"\n",
    "# save_model(label)\n",
    "restore_model(label)\n",
    "sX, sy = build_submodelD()\n",
    "\n",
    "for i in range(len(sX)):\n",
    "    train_submodel(i+1, np.array(sX[i]), sy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sX)):\n",
    "    print(len(sX[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #training m1\n",
    "\n",
    "# feed_dict={env.x_m1: imgs, env.y_m1: imgs_y, env.training: True}\n",
    "# sess.run(getattr(env, 'optimizer_m1'), feed_dict)\n",
    "# print(sess.run(getattr(env, 'acc_m1'), feed_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training 1th model.\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating00\n",
      "acc: 0.51172\n",
      "Evaluating100\n",
      "acc: 0.51172\n"
     ]
    }
   ],
   "source": [
    "imgs = X_train[:4000]\n",
    "imgs = np.reshape(imgs, [-1, 3072])\n",
    "imgs_y = np.array([[0,1]]*2000 + [[1,0]]*2000, np.float32)\n",
    "\n",
    "np.random.seed(42)\n",
    "ind = np.arange(0, imgs.shape[0])\n",
    "ind = np.random.shuffle(ind)\n",
    "imgs, imgs_y = imgs[ind][0], imgs_y[ind][0]\n",
    "\n",
    "train_submodel(1, imgs, imgs_y, n_epoch= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    print(\"with tf.variable_scope('sub_model_{0}'):\\n    inp =\\n    shape_m{0} = [inp, int(sqrt(inp)), 2]\\n    env.x_m{0}, env.y_m{0}, env.optimizer_m{0}, env.preds_m{0}, env.acc_m{0} = sub_attack_model(shape_m{0})\\n\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "en = [activations[0], activation_grads[0], model_grads[0]]\n",
    "print (len(en))\n",
    "print (len(en[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "activation_grads = [item[0] for item in ag] # gets the first item in each sublist.\n",
    "\n",
    "print(len(activation_grads))\n",
    "print(len(activation_grads[0]))\n",
    "print(len(activation_grads[0][0]))\n",
    "print(len(activation_grads[0][0][0]))\n",
    "# print(len(activation_grads[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def main_func(X,y,model_label):\n",
    "#     restore_model(model_label)\n",
    "#     Xflip = create_flip_wrt_class(X,y)\n",
    "#     return Xflip\n",
    "\n",
    "\n",
    "# n = 10\n",
    "# model_label=\"cifar_with_cnn\" + str(200)\n",
    "# X = X_train[:n]\n",
    "# y = y_train[:n]\n",
    "# X_flip = main_func(X,y,model_label)\n",
    "\n",
    "# flip_label = \"flipped_examples\"\n",
    "# save_as_txt(get_flip_path(flip_label), X_flip)\n",
    "# X_flip_load = load_from_txt(get_flip_path(flip_label))\n",
    "\n",
    "# # print(X_flip == X_flip_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "obj = Dummy()\n",
    "\n",
    "obj.hi = \"yash\"\n",
    "obj.bye = \"yash\"\n",
    "\n",
    "a = \"bye\"\n",
    "print(getattr(obj,a))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
